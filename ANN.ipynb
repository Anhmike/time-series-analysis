{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network basics\n",
    "\n",
    "The very basic ingredient of any artificial neural network is the artificial neuron, called <strong>Perceptron</strong>. These neurons are connected via <strong>synapses</strong>. Synapses take the input $x_{ij}$ of a preceding neuron and multiply his value with a weight $w_{ij}$ and output, $x_{ij} w_{ij}$ the result to the next subsequent neuron in.\n",
    "\n",
    "Neuron\n",
    "\n",
    "The neural network is not capable of deciding the size and depth of a neural network. What a ANN does learn are the models parameters, the weights on the synapses.\n",
    "\n",
    "Data moves through the network by a method called <strong>forward propagation</strong>, hence, such models are called <strong>feed forward networks</strong>\n",
    "\n",
    "The weights and inputs are placed in matrices $X$ and $W^{(1)}$\n",
    "\n",
    "where $Z^{(2)}$ is the calculated activity of the second alyer, that is $Z^{(2)}$ is the sum of weighted inputs from each neuron.\n",
    "\n",
    "$$\n",
    "z^{(2)} = X W^{(1)}\n",
    "$$\n",
    "\n",
    "\n",
    "The activation function has the form\n",
    "\n",
    "$$\n",
    "a(z) = \\frac{1}{1 + e^{(-z)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{(2)} = f(z^{(2)})\n",
    "$$\n",
    "\n",
    "What remains to be done is to multiply $a^{(2)}$ by the second layer weights $W^{(2)}$ \n",
    "\n",
    "$$\n",
    "z^{(3)} = a^{(2)} W^{(2)}\n",
    "$$\n",
    "\n",
    "and apply another activation function to $z^{(3)}$ to yield the estimate of the test score $\\hat{y}$\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(z^{(3)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ANN(object):\n",
    "    def __init__(self):\n",
    "        # hyperparameters\n",
    "        self.inputLayerSize = 2\n",
    "        self.hiddenLayerSize = 3\n",
    "        self.outputLayer.Size = 1\n",
    "        \n",
    "        # initial weights\n",
    "        self.W1 = np.random.randn(self.inputLayerSize, self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize, self.outputLayerSize)\n",
    "        \n",
    "    def sigmoid(z, prime=False):\n",
    "        # apply sigmoid activation function\n",
    "        # activation is applied elemnt wise and return a result with the same dimension as it is iven\n",
    "        if prime == False:\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        else:\n",
    "            return np.exp(-z) / ((1 + np.exp(-z)) ** 2)\n",
    "        \n",
    "    def forward_propagation(self, X):\n",
    "        # prpagate input data through the network\n",
    "        self.z2 = np.dot(X, self.W1)\n",
    "        self.a2 = self.activation(self.z2)\n",
    "        self.z3 = np.dot(self.a2, self.W2)\n",
    "        yHat = self.activation(self.z3)\n",
    "        return yhat\n",
    "    \n",
    "    def cost_function(self, X, y, prime=False):\n",
    "        if prime = False:\n",
    "            #compute cost for given X, y\n",
    "            self.yHat = self.forward(X)\n",
    "            J = 0.5 * sum((y - self.yHat)) ** 2\n",
    "            return J\n",
    "        else:\n",
    "            # compute derivative with respect to W1 and W2\n",
    "            self.yHat = self.forward(X)\n",
    "            # hidden layer\n",
    "            delt3 = np.multiply(-(y - self.Hat), self.sigmoid(self.z3, prime=True))\n",
    "            dJdW2 = np.dot(self.a2.T, delta3)\n",
    "            # input layer\n",
    "            delta2 = np.dot(delta3, self.W2.T) * self.sigmoid(self.z2, prime=True)\n",
    "            dJdW1 = np.dot(X.T, delta2)\n",
    "            return dJdW1, dJdW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the <code>ANN</code> class to predict will yiel a bas result, since it has yet to be trained properly. The network is trained using a method called <strong>gradient descent</strong> and <strong>back propagation</strong>. To quantify the extend of how much the prediction missed the actual value a <strong>loss function</strong> is needed. A common loss function is the sum of squared errors\n",
    "\n",
    "$$\n",
    "J = \\sum_{i=1}^n\\frac{1}{2} e^2 = \\sum_{i=1}^n\\frac{1}{2}\\left(y - \\hat{y}\\right)^2\n",
    "$$\n",
    "\n",
    "The smaller the loss on the data, the better the estimate, hence this corresponds to a minimizing the loss function (training the network). Remember that $\\hat{y}$ is ultimately a function of all model data inputs and all model synapses weights. The only way to minimize the loss is by finding the optimal values for all included weights. By substituting $\\hat{y}$ with all above functions\n",
    "\n",
    "$$\n",
    "J =  \\sum_{i=1}^n \\frac{1}{2} \\left( y - f(f(XW^{(1)}) W^{(2)}) \\right)^2\n",
    "$$\n",
    "\n",
    "The contribution of the weighted inputs to the total error are found by the partial derivatives of the loss function with respect to the weights, the <strong>gradient</strong>. This method is called <strong>gradient descent</strong>. More specifically <strong>batch gradient descent</strong> will be applied here, as the all model weights are updated simultaneously\n",
    "\n",
    "However, if the cost function is non-convex, the gradient descent algorithm migth get stuck in local minima instead of the desired global minimum. For that matter, the sum of squared errors is chosen as the loss function as this is a convex function.\n",
    "\n",
    "The total error then needs to be propagated back through the system in accordance to the contribution of each preceding connected neuron.\n",
    "\n",
    "The $W$ is multiplied by the <strong>learning rate</strong> $\\nu$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
